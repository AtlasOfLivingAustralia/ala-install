# Deployment specific configuration for AWS spark quoll cluster.
# This file should be copied over `la-pipelines-local.yaml` when deployed
# so that it is used by the scripts.
#
# Long term, this configuration should be moved out of the project and should be generated
# by ansible scripts or as part of the debian package installation.
#
run:
  # where to run: local, spark-embedded or spark-cluster
  platform: local
  local:
    # jar: we get the jar from our dev or production environment
    sparkTmp: {{ spark_tmp }}
    sparkMaster: ""
    dwcaTmp: {{ dwca_tmp }}
    dwcaImportDir: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ dwca_import_dir }}
  spark-embedded:
    # jar: we get the jar from our dev or production environment
    sparkTmp: {{ spark_tmp }}
    sparkMaster: ""
    dwcaTmp: {{ dwca_tmp }}
    dwcaImportDir: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ dwca_import_dir }}
  spark-cluster:
    jar: /usr/share/la-pipelines/la-pipelines.jar
    sparkTmp: {{ spark_tmp }}
    sparkMaster: spark://{{ spark_master }}:7077
    dwcaTmp: {{ dwca_tmp }}
    dwcaImportDir: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ dwca_import_dir }}


collectory:
  wsUrl: {{ collectory_url }}
  httpHeaders:
    Authorization: {{ ala_api_key }}

imageService:
  wsUrl: {{ image_service_url }}
  httpHeaders:
    apiKey:  {{ ala_api_key }}

general:
  attempt: 1
  hdfsSiteConfig: {{ hadoop_install_dir }}/etc/hadoop/hdfs-site.xml
  coreSiteConfig: {{ hadoop_install_dir }}/etc/hadoop/core-site.xml
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
  targetPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
dataset-validated-dump:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
dataset-count-dump:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
dataset-archive-list:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ dwca_import_dir }}/
dwca-avro:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ dwca_import_dir }}/{datasetId}/{datasetId}.zip
  tempLocation: {{ spark_tmp }}/dwca-avro/{datasetId}
  hdfsTempLocation: {{ spark_tmp }}
interpret:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}/{datasetId}/1/verbatim.avro
images:
  tempLocation: {{ spark_tmp }}
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
  targetPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
export-latlng:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
uuid:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
sample:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
sample-avro:
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
validation-report:
  checkSolr: {{ check_solr }}
  checkSampling: {{ include_sampling }}
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
  zkHost: {{ zk_host | default('localhost:2181') }}
index:
  includeSampling: {{ include_sampling }}
  inputPath: hdfs://{{ hadoop_master }}:{{ hadoop_port }}{{ pipelines_data_dir }}
  zkHost: {{ zk_host | default('localhost:2181') }}
