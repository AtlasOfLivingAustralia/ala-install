<?xml version="1.1" encoding="UTF-8" standalone="no"?><flow-definition plugin="workflow-job@2.40">
  <actions>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobAction plugin="pipeline-model-definition@1.7.2"/>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction plugin="pipeline-model-definition@1.7.2">
      <jobProperties/>
      <triggers/>
      <parameters/>
      <options/>
    </org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction>
  </actions>
  <description>&#13;
    Interpret all datasets.&#13;
    This pipeline job split large and small datasets.&#13;
  </description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <org.jenkinsci.plugins.workflow.job.properties.DisableConcurrentBuildsJobProperty/>
    
  </properties>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsFlowDefinition" plugin="workflow-cps@2.83">
    <script>
stage('Dump out dataset counts') {
 node('master'){
     sh 'sudo -u spark la-pipelines dataset-list'
 }
}

def datasets = getDatasets()

// set up for 
def names = nodeNames()
def batches = batchDatasets(datasets, names.size())
def branches = [:]  

batches.eachWithIndex { batch, index -&gt;
  def nodeName = names[index.mod(names.size())]  
  branches["node_" + nodeName] = {
    node(nodeName) {
      echo 'Datasets ' + batch
      if (batch){
          build job: 'Image-load', parameters: [
               new org.jvnet.jenkins.plugins.nodelabelparameter.NodeParameterValue
                   ("TARGET_NODE", "description", nodeName),
               string(name: 'datasetId', value: batch),
               string(name: 'mode', value: '--embedded')
          ]
      }
    }
  } 
}

stage('Run image load on jenkins nodes') {
  parallel branches
}

@NonCPS def batchDatasets(datasets, noOfBatches){
    //split into 6 batches
    def batches = ["","","","","",""]
    datasets.eachWithIndex { datasetId, index -&gt; 
        batches[index.mod(noOfBatches)] = datasetId + " " + batches[index.mod(noOfBatches)]
    }
    return batches
}

@NonCPS def getDatasets(){
  def file = new File("/tmp/dataset-counts.csv")

  def datasets = []

  def line, noOfLines = 0;
  file.withReader { reader -&gt;
    while ((line = reader.readLine()) != null) {
      def datasetId = line.split(",")[0]
      def recordCount = line.split(",")[1]
      datasets.add(datasetId)  
    }
  }   
  return datasets
}

// This method collects a list of Node names from the current Jenkins instance 
@NonCPS def nodeNames() { 
  return jenkins.model.Jenkins.instance.nodes.collect { node -&gt; node.name } 
}

</script>
    <sandbox>true</sandbox>
  </definition>
  <triggers/>
  <disabled>false</disabled>
</flow-definition>