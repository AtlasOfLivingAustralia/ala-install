<?xml version="1.1" encoding="UTF-8" standalone="no"?><flow-definition plugin="workflow-job@2.39">
  <actions>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobAction plugin="pipeline-model-definition@1.7.1"/>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction plugin="pipeline-model-definition@1.7.1">
      <jobProperties/>
      <triggers/>
      <parameters/>
      <options/>
    </org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction>
  </actions>
  <description>&#13;
    Index all datasets.&#13;
    This pipeline job split large and small datasets.&#13;
  </description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <org.jenkinsci.plugins.workflow.job.properties.DisableConcurrentBuildsJobProperty/>

  </properties>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsFlowDefinition" plugin="workflow-cps@2.82">
    <script>stage('Dump out dataset counts') {
 node('master'){
     sh 'sudo -u spark la-pipelines validation-list'
 }
}

stage('Clear index') {
 node('master'){
     sh 'curl -X GET "http://aws-solrcloud-quoll-1.ala:8983/solr/admin/collections?action=DELETE&amp;name=biocache"'
     sh 'curl -X GET "http://aws-solrcloud-quoll-1.ala:8983/solr/admin/collections?action=CREATE&amp;name=biocache&amp;numShards=8&amp;maxShardsPerNode=1&amp;replicationFactor=1&amp;collection.configName=biocache"'
 }
}

def datasets = splitDatasetsBySize()
def smallDatasets = datasets[0]
def largeDatasets = datasets[1]

// def largeDatasets = ['dr893', 'dr9967', 'dr6687']

stage('Index large datasets on cluster') {
    echo 'Index large datasets on cluster'
    node('master'){
     largeDatasets.each { datasetId -&gt;
        echo 'Large dataset for cluster: ' + datasetId
        build job: 'Index-dataset', parameters: [
             string(name: 'datasetId', value: datasetId),
             string(name: 'mode', value: '--cluster')
        ]        
     }
    }
}

// set up for 
def names = nodeNames()
def batches = batchDatasets(smallDatasets, names.size())
def branches = [:]  

batches.eachWithIndex { batch, index -&gt;
  def nodeName = names[index.mod(names.size())]  
  branches["node_" + nodeName] = {
    node(nodeName) {
      echo 'Datasets ' + batch
      build job: 'Index-dataset', parameters: [
           new org.jvnet.jenkins.plugins.nodelabelparameter.NodeParameterValue
               ("TARGET_NODE", "description", nodeName),
           string(name: 'datasetId', value: batch),
           string(name: 'mode', value: '--local')
      ]
    }
  } 
}

stage('Index small datasets on jenkins nodes') {
  parallel branches
}


@NonCPS def batchDatasets(datasets, noOfBatches){
    //split into 6 batches
    def batches = ["","","","","",""]
    datasets.eachWithIndex { datasetId, index -&gt; 
        batches[index.mod(noOfBatches)] = datasetId + " " + batches[index.mod(noOfBatches)]
    }
    return batches
}

@NonCPS def splitDatasetsBySize(){
  def file = new File("/tmp/dataset-validation-list.csv")

  def small = []
  def large = []

  def datasetGroups = [small, large]

  def line, noOfLines = 0;
  file.withReader { reader -&gt;
    while ((line = reader.readLine()) != null) {
      def datasetId = line.split(",")[0]
      def recordCount = line.split(",")[1]
      if (recordCount.toLong() &gt; 300000){
        large.add(datasetId)  
      } else {
        small.add(datasetId)
      }
    }
  }   
  return datasetGroups
}

// This method collects a list of Node names from the current Jenkins instance 
@NonCPS def nodeNames() { 
  return jenkins.model.Jenkins.instance.nodes.collect { node -&gt; node.name } 
}

</script>
    <sandbox>true</sandbox>
  </definition>
  <triggers/>
  <disabled>false</disabled>
</flow-definition>